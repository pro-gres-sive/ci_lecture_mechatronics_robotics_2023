{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940cf0da-edae-45b2-a447-b1b938f3fb22",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Try out the given examples. Change parameters such as learning rate, number of epochs and observe the NNs behavior in training\n",
    "2. Change the parameters and the topology in the overfitting example and try to find the most simple NN suitable - avoid overfitting\n",
    "3. Implement the ReLU as a new activation function and create and train a NN to regress the data in the file \"regression_example_2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2d7b7-0fa3-4dd8-b51f-016ea5c05a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #library for visualizing data\n",
    "%matplotlib widget \n",
    "#setting for jupyter lab\n",
    "plt.rcParams['figure.figsize'] = [10, 5] #setting figure size (plots)\n",
    "\n",
    "import pandas as pd  # (software library for data analysis and manipulation, https://pandas.pydata.org/docs/)\n",
    "import numpy as np  # (software library for matrix multiplications, https://numpy.org/doc/)\n",
    "import statistics as stats  # (python module for statistic calculations, https://docs.python.org/3/library/statistics.html)\n",
    "import time #python time module\n",
    "import random #module for generation of random values\n",
    "\n",
    "def lin(x):\n",
    "    return x\n",
    "\n",
    "def lin_deriv(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "\n",
    "    history = []\n",
    "    layer_topology = []\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, layers, activation_function='tanh', print_model_info=True):\n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_deriv\n",
    "        elif activation_function == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_deriv\n",
    "        elif activation_function == 'lin':\n",
    "            self.activation = lin\n",
    "            self.activation_prime = lin_deriv\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        # Set weights for each layer\n",
    "        self.weights = []\n",
    "        for i in range(0, len(layers) - 1):\n",
    "            additional_column = 1 if i < len(layers) - 2 else 0\n",
    "            r = np.random.randint(-500, 500, (layers[i] + 1, layers[i + 1] + additional_column)) * 0.001 #set to values between -0.5 and 0.5\n",
    "            self.weights.append(r)\n",
    "\n",
    "        if print_model_info:\n",
    "            self.print_model_info()\n",
    "\n",
    "            \n",
    "            \n",
    "    def print_model_info(self):\n",
    "        print('Model: \\n')\n",
    "\n",
    "        layers = self.layers\n",
    "        max_nodes = max(layers)\n",
    "        for i, layer in enumerate(layers):\n",
    "            diff = abs(max_nodes - layer)\n",
    "            output = ''\n",
    "            for k in range(round(diff + 0.5 / 2)):\n",
    "                output += ' '\n",
    "            for j in range(layer):\n",
    "                output += ' O'\n",
    "            for k in range(round(diff + 5)):\n",
    "                output += ' '\n",
    "            if i == 0:\n",
    "                output += f'{layer} Input(s)'\n",
    "            elif i == len(layers) - 1:\n",
    "                output += f'{layer} Output(s)'\n",
    "            else:\n",
    "                output += f'{layer} Node(s)'\n",
    "            print(output)\n",
    "\n",
    "        print('\\nActivation:', str(self.activation))\n",
    "\n",
    "        weight_count = 0\n",
    "        for weight in self.weights:\n",
    "            weight_count += weight.shape[0] * weight.shape[1]\n",
    "\n",
    "        print(f'{len(layers)} Layers')\n",
    "        print(f'{weight_count} trainable parameters.\\n\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, l_rate=0.2, epochs=100000, print_freq=100, validation_x=None, validation_y=None):\n",
    "        t_0 = time.perf_counter()\n",
    "\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.hstack((ones.T, X))  # stack ones for bias\n",
    "\n",
    "        history = []\n",
    "\n",
    "        for k in range(epochs):\n",
    "            t_0_epoch = time.perf_counter()\n",
    "            for i, row in enumerate(X):  # loop through every training data sample\n",
    "\n",
    "                # 1 FEED FORWARD - loop through the layers and compute layer activations\n",
    "                layer_activations = [row] #create list for layer activations and use present NN inputs as first entry\n",
    "                for w_i in range(len(self.weights)):\n",
    "                    dot_value = np.matmul(layer_activations[w_i], self.weights[w_i]) #weighted sum\n",
    "                    activation = self.activation(dot_value)\n",
    "                    layer_activations.append(activation)\n",
    "\n",
    "                # 2: Output Layer\n",
    "                nn_result = layer_activations[-1]  # last item in our layer_activations is the output of our network\n",
    "                error = y[i] - nn_result\n",
    "\n",
    "                deltas = [error * self.activation_prime(layer_activations[-1])] #create list for error signals and store error signal of output layer\n",
    "\n",
    "                # 3: Backpropagation through hidden layers\n",
    "                for la_i in range(len(layer_activations) - 2, 0, -1):  # iterate backwards through layer activations and weights\n",
    "                    ##compute delta based on previous examined layers deltas and store in list\n",
    "                    deltas.append(np.matmul(deltas[-1], self.weights[la_i].T) * self.activation_prime(layer_activations[la_i]))\n",
    "                deltas.reverse()  # invert order of delta list\n",
    "\n",
    "                # 4: Gradient calculation and weight update\n",
    "                for w_i in range(len(self.weights)):\n",
    "                    layer_activation = np.atleast_2d(layer_activations[w_i])\n",
    "                    delta = np.atleast_2d(deltas[w_i])\n",
    "                    gradient = np.matmul(layer_activation.T, delta)\n",
    "                    self.weights[w_i] += l_rate * gradient\n",
    "\n",
    "            # performance control\n",
    "            Y_pred = nn.predict(X[:, 1:])\n",
    "            error = y.reshape(len(y), 1) - Y_pred\n",
    "            mse = (error ** 2).mean()\n",
    "            mae = np.abs(error).mean()\n",
    "            \n",
    "            mse_v = mae_v = None\n",
    "            \n",
    "            if validation_x is not None:\n",
    "                y_val_pred = nn.predict(validation_x)\n",
    "                val_error = validation_y.reshape(len(validation_y), 1) - y_val_pred\n",
    "                mse_v = (val_error ** 2).mean()\n",
    "                mae_v = np.abs(val_error).mean()\n",
    "\n",
    "            history.append([k, mse, mae, mse_v, mae_v])\n",
    "\n",
    "\n",
    "            # print output\n",
    "            if k % print_freq == 0:\n",
    "                t_1_epoch = time.perf_counter()\n",
    "                time_delta_epochs = t_1_epoch - t_0_epoch\n",
    "                mean_time_delta_epochs_ms = round((time_delta_epochs / print_freq) * 1000, 4)\n",
    "                print(f'Epochs: {k}  -  MSE: {round(mse,4)}  -  MAE: {round(mae,4)}  -  Time dur. per epoch: {mean_time_delta_epochs_ms} ms')\n",
    "\n",
    "        self.history = history\n",
    "        print(f'\\nTraining finished. Time consumed: {round(time.perf_counter() - t_0, 2)} s')\n",
    "        return history\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        prediction = np.hstack((ones.T, X))\n",
    "\n",
    "        for weight_matrix in self.weights:\n",
    "            prediction = self.activation(np.matmul(prediction, weight_matrix))\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ab7f2-a527-4a8e-b162-5dab29ce2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic gate examples\n",
    "xor_gate = pd.DataFrame([[0,0,0],[1,0,1],[0,1,1],[1,1,0]], columns = ['x1', 'x2', 'y'])\n",
    "and_gate = pd.DataFrame([[1,1,1],[1,0,0],[0,1,0],[0,0,0]], columns = ['x1', 'x2', 'y'])\n",
    "or_gate = pd.DataFrame([[1,1,1],[1,0,1],[0,1,1],[0,0,0]], columns = ['x1', 'x2', 'y'])\n",
    "\n",
    "gate = or_gate\n",
    "X = gate[['x1', 'x2']].to_numpy()\n",
    "y = gate[['y']].to_numpy()\n",
    "nn = NeuralNet([2,1], activation_function='tanh')\n",
    "history = nn.fit(X, y, l_rate=0.95, epochs = 200, print_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e66aba-94c3-40df-9870-348eb3eff536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.predict(np.array([0,0])))\n",
    "print(nn.predict(np.array([1,0])))\n",
    "print(nn.predict(np.array([1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f941bb9-98d8-4d28-8df0-d7e0fb4bba94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#regression example\n",
    "data = pd.read_csv('./regression_example_1.csv')\n",
    "\n",
    "training_data = data.sample(frac=0.8)\n",
    "validation_data = data.drop(training_data.index)\n",
    "\n",
    "X = training_data[['input']].to_numpy()\n",
    "y = training_data[['output']].to_numpy()\n",
    "X_val = validation_data[['input']].to_numpy()\n",
    "y_val = validation_data[['output']].to_numpy()\n",
    "\n",
    "nn = NeuralNet([1,2,3,1])\n",
    "\n",
    "history = nn.fit(X, y, l_rate=0.075, epochs = 1000, print_freq=250, validation_x = X_val, validation_y = y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0907b67-f952-4d3d-a81b-17f62f54f95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#overfitting example\n",
    "\n",
    "#set seed for repeatable results\n",
    "seed=24253\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "data = pd.read_csv('./regression_example_2.csv')\n",
    "\n",
    "training_data = data.sample(frac=0.8)\n",
    "\n",
    "while [0.12] not in training_data['input'].values or [0.13] not in validation_data['input'].values:\n",
    "    training_data = data.sample(frac=0.8)\n",
    "    validation_data = data.drop(training_data.index)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = training_data[['input']].to_numpy()\n",
    "y = training_data[['output']].to_numpy()\n",
    "X_val = validation_data[['input']].to_numpy()\n",
    "y_val = validation_data[['output']].to_numpy()\n",
    "nn = NeuralNet([1,3,6,6,19,14,6,6,3,1], activation_function='tanh')\n",
    "\n",
    "history = nn.fit(X, y, l_rate=0.09, epochs = 8000, print_freq=250, validation_x = X_val, validation_y = y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44730b-1040-492d-8af4-aa2f1cdd975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot history\n",
    "plt.close('all')\n",
    "history_df = pd.DataFrame(history, columns=['epoch', 'MSE', 'MAE', 'MSE_val', 'MAE_val'])\n",
    "history_df.plot(x='epoch', y=['MSE', 'MSE_val'])\n",
    "history_df.tail(int(len(history_df.index)/1.5)).plot(x='epoch',  y=['MSE', 'MSE_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e12579-8f46-41bc-b9c8-1dc602715133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test data\n",
    "X_test = np.atleast_2d(np.linspace(0,1,1001,1)).T\n",
    "Y=nn.predict(X_test)\n",
    "\n",
    "predictions = pd.DataFrame([X_test.flatten(),Y.flatten()]).T\n",
    "predictions.columns = ['input', 'output']\n",
    "\n",
    "training_data = pd.DataFrame([X.flatten(),y.flatten()]).T\n",
    "training_data.columns = ['input', 'output']\n",
    "\n",
    "validation_data = pd.DataFrame([X_val.flatten(),y_val.flatten()]).T\n",
    "validation_data.columns = ['input', 'output']\n",
    "\n",
    "#display predictions\n",
    "ax = training_data.plot(kind='scatter', x= 'input', y='output', label='training data')\n",
    "validation_data.plot(kind='scatter', x= 'input', y='output', ax=ax, color='red', label='validation data')\n",
    "predictions.plot(kind='line', x= 'input', y='output', ax = ax, color='orange')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e1785-7f69-45bd-ba0e-12f373d7eefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
